# -*- coding: utf-8 -*-
"""skin_cancer_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B9dK7stWRfFj7CHG23M3l8_1J5n33gaL

#            Skin Cancer Detection

  Develop practical skills in applying Deep Learning for medical image classification. The tasks
focus on Skin Cancer Detection using CNNs. Internees will
preprocess datasets, apply data augmentation, and fine-tune pre-trained models for accurate
classification.
  # Import Required Libraries
"""

# For data manipulation and visualization
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# For image processing
import os
import cv2

# For deep learning
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50, EfficientNetB0

# For metrics and performance evaluation
from sklearn.metrics import classification_report, confusion_matrix

# For working with files
import zipfile
from google.colab import files

"""# Upload and Extract the Dataset

upload the dataset from the machine
"""

# Step 1: Upload ZIP File
from google.colab import files

uploaded = files.upload()  # This will open a file upload dialog in Colab

"""# Extract the ZIP file:"""

# Step 2: Extract ZIP File
import zipfile
import os

# Get the uploaded ZIP file name
zip_file_name = list(uploaded.keys())[0]

# Define the extraction directory
extract_dir = './dataset'

# Extract the ZIP file
with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# Confirm extraction
print("Files extracted to:", extract_dir)
print("Contents:", os.listdir(extract_dir))

"""# Check Dataset Structure"""

# Check the main directory structure
import os

print("Main Directory Contents:")
print(os.listdir('./dataset'))

# If there are subfolders, list their contents
for folder in os.listdir('./dataset'):
    folder_path = os.path.join('./dataset', folder)
    if os.path.isdir(folder_path):
        print(f"\nContents of {folder}:")
        print(os.listdir(folder_path))

"""# Data Preprocessing:
○ Load the dataset, normalize pixel values to [0, 1].
○ Resize images to a fixed size (e.g., 224x224 for compatibility with CNNs).
○ Split the dataset into training, validation, and testing sets.
# Load and Inspect Metadata
"""

# Load metadata
import pandas as pd

metadata_path = './dataset/metadata.csv'
metadata = pd.read_csv(metadata_path)

# Display the first few rows
print("Metadata Preview:")
print(metadata.head())

# Check the columns
print("\nColumn Names:")
print(metadata.columns)

"""# Data Preparation

**1. Prepare the Data**

Append .jpg to isic_id to get full image filenames.
Use diagnosis_1 to label images as binary (e.g., Malignant=1 and others=0).
**2. Normalize and Resize**

Load images using OpenCV or PIL, normalize pixel values to [0, 1], and resize them to 224x224.

**3. Split the Dataset**

Split the dataset into training (70%), validation (15%), and testing (15%).
"""

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split

# Define paths
dataset_dir = './dataset/'
image_dir = dataset_dir  # Images are in the root directory

# Add .jpg to isic_id and create full image paths
metadata['image_path'] = metadata['isic_id'] + '.jpg'
metadata['image_path'] = metadata['image_path'].apply(lambda x: os.path.join(image_dir, x))

# Convert diagnosis_1 to binary labels (Malignant=1, others=0)
metadata['label'] = metadata['diagnosis_1'].apply(lambda x: 1 if x == 'Malignant' else 0)

# Check the updated metadata
print("Updated Metadata:")
print(metadata[['image_path', 'label']].head())

"""# Normalize, Resize, and Split the Dataset

we’ll load the images, normalize pixel values to [0, 1], resize them to 224x224, and split the dataset into training, validation, and testing sets.


"""

# Parameters
IMG_SIZE = 224  # Resize all images to 224x224
test_size = 0.15  # 15% for testing
val_size = 0.15  # 15% for validation from the remaining train set

# Load, normalize, and resize images
def preprocess_image(image_path):
    try:
        img = cv2.imread(image_path)  # Read the image
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB
        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))  # Resize to 224x224
        img = img / 255.0  # Normalize pixel values to [0, 1]
        return img
    except Exception as e:
        print(f"Error processing image: {image_path}, {e}")
        return None

# Load images and labels
images = []
labels = []

for idx, row in metadata.iterrows():
    img = preprocess_image(row['image_path'])
    if img is not None:
        images.append(img)
        labels.append(row['label'])

# Convert to NumPy arrays
images = np.array(images)
labels = np.array(labels)

# Split the dataset
X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=(test_size + val_size), random_state=42, stratify=labels)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(test_size / (test_size + val_size)), random_state=42, stratify=y_temp)

# Print dataset shapes
print(f"Training set: {X_train.shape}, {y_train.shape}")
print(f"Validation set: {X_val.shape}, {y_val.shape}")
print(f"Test set: {X_test.shape}, {y_test.shape}")

# Select only the relevant columns for splitting
metadata = metadata[['image_path', 'label']]

# Split the metadata into training, validation, and test sets
train_metadata, temp_metadata = train_test_split(metadata, test_size=0.4, stratify=metadata['label'])
val_metadata, test_metadata = train_test_split(temp_metadata, test_size=0.5, stratify=temp_metadata['label'])

# Check the shape of each split
print(f"Training set: {train_metadata.shape}")
print(f"Validation set: {val_metadata.shape}")
print(f"Test set: {test_metadata.shape}")

"""# Convert labels to strings"""

train_metadata['label'] = train_metadata['label'].astype(str)
val_metadata['label'] = val_metadata['label'].astype(str)
test_metadata['label'] = test_metadata['label'].astype(str)

"""# Data Augmentation

Let's apply data augmentation (e.g., random rotation, flipping, zooming, and brightness adjustments) to help generalize the model and avoid overfitting
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Initialize the ImageDataGenerator for data augmentation
train_generator = train_datagen.flow_from_dataframe(
    dataframe=train_metadata,
    x_col='image_path',
    y_col='label',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

val_generator = val_test_datagen.flow_from_dataframe(
    dataframe=val_metadata,
    x_col='image_path',
    y_col='label',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

test_generator = val_test_datagen.flow_from_dataframe(
    dataframe=test_metadata,
    x_col='image_path',
    y_col='label',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

"""# Model Development

Now that the dataset is ready, we can move forward to developing the model using transfer learning with a pre-trained CNN (e.g., ResNet50, EfficientNet) for binary classification.
We'll replace the last layer to match our problem (binary classification).
# Compile and Train the Model

Use Binary Crossentropy as the loss function, Adam optimizer, and accuracy as a metric.
"""

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam

# Load the pre-trained ResNet50 model (without the top layer)
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers of the pre-trained model
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers on top for binary classification
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
x = Dense(1, activation='sigmoid')(x)  # Sigmoid for binary classification

# Create the final model
model = Model(inputs=base_model.input, outputs=x)

# Compile the model
model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Display the model summary
model.summary()

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    epochs=10,  # You can adjust this number
    validation_data=val_generator,
    validation_steps=val_generator.samples // val_generator.batch_size
)

"""# Fine-Tuning the Model

"""

# Unfreeze the top layers of the ResNet50 model
for layer in base_model.layers[-10:]:  # Unfreeze the last 10 layers, for example
    layer.trainable = True

# Recompile the model after unfreezing
model.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])

# Continue training
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    epochs=10,  # Continue for more epochs
    validation_data=val_generator,
    validation_steps=val_generator.samples // val_generator.batch_size
)

"""# Evaluating on the Test Set


After training for the desired number of epochs, you can evaluate the model on the test set to see how it generalizes to unseen data.
"""

test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)
print(f"Test Accuracy: {test_accuracy}")

"""# Visualizing Results

we can also plot the accuracy and loss curves to see the model's performance over the epochs.
"""

import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

"""# Evaluate

using accuracy, precision, recall, and F1-score.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Get the predictions from the model on the test dataset
test_predictions = model.predict(test_generator)
test_predictions = (test_predictions > 0.5).astype("int32")  # Convert probabilities to binary predictions

# Get the true labels for the test set
test_labels = test_generator.labels

# Evaluate the model
accuracy = accuracy_score(test_labels, test_predictions)
precision = precision_score(test_labels, test_predictions)
recall = recall_score(test_labels, test_predictions)
f1 = f1_score(test_labels, test_predictions)

# Print the evaluation results
print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1-Score: {f1:.4f}')

"""# Plotting Confusion Matrix (For Precision, Recall, F1-Score)


The confusion matrix is a great visualization to show how well the model is distinguishing between classes.
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import numpy as np

# Get confusion matrix
cm = confusion_matrix(test_labels, test_predictions)

# Plot confusion matrix using seaborn
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""# Plotting Precision-Recall Curve


To plot the precision-recall curve, you can use sklearn's precision_recall_curve.
"""

from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

# Get the precision-recall curve data
precision, recall, thresholds = precision_recall_curve(test_labels, test_predictions)

# Plot the curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label='Precision-Recall Curve')
plt.title('Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.show()

"""# Plotting ROC Curve and AUC"""

from sklearn.metrics import roc_curve, auc

# Get ROC curve data
fpr, tpr, _ = roc_curve(test_labels, test_predictions)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()